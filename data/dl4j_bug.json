{"ids": ["p6QenuSC", "FPslUV31", "wxhVpEdI", "GKtVhXDg", "mwmRQtmu", "tfhTWYKt", "BlZzM5tZ", "IGMFeFSz", "h2eqQda3", "LIUOVYK6", "gPe7FuNk", "N1vqKyg6", "aqmnZesI", "ZYNB4bqz", "YKS5Nsq9", "YZGMx9tH"], "dialog": [["I thought someone at DL4J would like to know:  The PredictGenderTrain  dl4j example frequently does not learn/converge (~50% error through all epochs).   It seems to work reliably when I reduce the   learningRate = 0.005;//GF was .01      So, you might update source code.", "gforman44: pull request would be even better :D", "FYI: when it doesn\\'t converge, it repeatedly gets:  \"o.d.optimize.solvers.BaseOptimizer - Hit termination condition on iteration 0\" errors.", "agibsonccc: Sorry, I'm not that sophisticated with GIT yet.", "[<-LINK->]  doesn't hurt to learn :D it's only a few commands [<-LINK->] It's widely documented", "agibsonccc: OK, wow, I did it as a pull request.  That wasn't as hard as I thought.   You can quote me on that for future people.", "haha awesome. congrats! :D. Merged :D welcome to the world of open source", "This is awesome.", "Thanks for giving me a push, Adam. I mean a pull. Or whatever.", "heh"], ["Is there an env variable that I can use to specify which BLAS to use (MKL vs openblas)?", "akhodakivskiy: If MKL is in your library path, it will get used, but you can disable that by setting the java.library.path system property to an empty string", "like this? -Djava.library.path=\"\"", "Yes, that's one way", "Hm, doesn't quite work.  works like this though -$ LD_LIBRARY_PATH=\"\" java Main", "There was this bug with [<-CODE->] have you guys had a chance to fix it in master?", "I'm trying to compare MKL vs OpenBLAS performance in my case", "Sure, it's only a build issue with OpenBLAS [<-LINK->] ", "So when I build from source - does the build also discover MKL automatically and link against it unconditionally?", "Does this happen when building libnd4j or nd4j?", "lib", "In my use case OpenBLAS is 3-4 times faster than MKL", "surprising.", "Yeah.."], ["Investigating why a Keras (theano backend) model and the (supposed to be) equivalent dl4j model imported using your new import functionality does not agree on class probabilities for a single validation image. When importing the network config and weights in dl4j I get the following warnings [<-LINK->]. Could these be relevant for the differing model evaluations (I am ignoring training for now).", "@perceptoid dropout is the likely culprit. add output(input,false) to your network. the false is test mode", "@agibsonccc dont quite follow. Change the keras network or dl4j network after import?", "I'm saying when you call output on your neural net. add false to the parameter as a parameter", "ahh, ok", "@agibsonccc adding false to model.output gives me the same result."], ["Uhm, guys, this should work right? Nd4j.ones(2, 2).eq(1); To get a boolean matrix by element wise comparison to a scalar", "yup.eq is 0/1", "Okay, because it throws exceptions. I'll create an issue ", "thanks", "It works on neither cuda nor native, both with different exception messages", "I've debugged it, and it really seems like bugs"], ["Hey, sorry for interrupting you, but I got a quick question, wondering if someone can answer it... If I run the following line of code on a pretty large Nd4j array (no dl4j involved) with tons of negative infinity values in it, a lot of messages saying \"Number: -Infinity\" get logged. Is this intentional, and if so, why? [<-CODE->]", "StillNoNumber: probably just a bug, leaked debug messsage", "StillNoNumber: i\u2019ll check that right now", "yes, because i've wrote that one", ":)", "raver119: Assumed so. Thanks anyway, should I file a report or something?", "StillNoNumber: thanks, no need, i\u2019m already opening nd4j", "Alright :D", "i mean - you\u2019ve already reported it :)", "yeah but you might prefer it if I report it somewhere so you can organize your to-do-list or whatever", "raver119: Found it, line 238 in BooleanIndexing in case you haven't yet", ":P"], ["I am trting to load a saved keras model, and the backend is tensorflow. My environment is eclipse, java 1.8, CPU. this is my code:[<-LINK->] and this is my pom file [<-LINK->] But it failed.  the error information is :[<-CODE->]. I also try to use the git source. there is the same error in the deeplearning4j-modelimport project:[<-CODE->]. How to solve this question?"], ["Hi Guys", "I've realized that my /tmp folder gets filled with tons of 'restoreXXXX' files, that I guess come from the use of the ModelSerializer.restoreMultiLayerNetwork(path) function. Is there a way to regulate that?", "app86: we can look in to that - could you file an !issue ?", "shouldn\u2019t be. i usually call deleteOnExit", "but i\u2019ll look into that now", "ah that\u2019s not me", "that\u2019s probably earlystopping you use there?", "I used early stopping for the trainning, yes", "right, do as adam said, file an issue", "Ok", "just file an issue, and we\u2019ll sort it out after release hits. it\u2019s not really a big deal anyway", "Ok."], ["I still don't know why I get this: Warning: class 1 was never predicted by the model.", "again, can you confirm that you had class 1 in your test set?", "I verified each step and my training data path seems ok", "yes..i have two folders. is face. no face", "enache2004: track your data input pipeline. there were many changes there.", "[<-CODE->] ", "enache2004: how about the model params? anything funky there?", "for nueral net ?", "yes", "everything is the same", "that's strange", "switching to 3.9 and works perfect", "this is my model", "enache2004: please don't do that", "use !gist.", "I like how is formatting :D", "ok", "[<-LINK->] ", "aha, another change applied is xavier weight init", "originally it was wrong", "now it's fixed", "original implementation is called XAVIER_LEGACY", ".weightInit(WeightInit.XAVIER) <----- this thing", "so ...I will changed to ? legacy? the other think that I noticed is StandardScaler which is deprecated", "could this change my results ?", "not sure what was changed for StandardScaler...", "i mean it's just new implementation available, but don't know what was wrong with original", "I don't know what to follow. to spend time to check if the latest version of dl4j has the native libraries fixed for linux or to keep my 3.9 version and investigate the part with java.library.path"], ["Hi! ", "We've tried DL4J with word embedding (Word2Vec). We found quite convenient to put the original words as metadata in the input dataset. (1) To the original sample classifier, in the DataFetcher we added: [<-CODE->]. (2) So when we evaluate the model, we can easily print the input/expected output/actual output: [<-CODE->]. (3) For that to work we had to override the BaseDataFetched initializeCurrFromList() as it didn't retain the metadatas. Perhaps this is a bug? [<-CODE->]. I hope this is what MetaData are for. If so, it would be good to patch the BaseDataFetcher with the missing curr.setExampleMetaData(metas). Also, it would be good to have MetaData in the DL4J samples.", "Thanks for the good work, DL4J people! Best,E.", "@namsor mind filing an !issue with this feedback?", "Done : [<-ISSUE->]", "thanks!"], ["I wonder whether there may be a small bug in spark:IterativeReduceFlatMap.java in the way the parameters are flatten/deflatten or re-injected in the network, but I'm not sure enough to fill a bug report. Because my DBN was training fine on 1 thread, but params were wrong with this spark approach. Then, I replaced with my own flattening/deflattening of the parameters, and now it's training fine. I've not been able to dig into the original code to spot the issue, if there is any, but well, this is just to let you know in case you hear about that later on... Anyway, plz don't loose time for now about that, it's probably just my fault :-) and thanks for these examples !", "I've made a gist with my proposed modifications: [<-LINK->]", "@cerisara Pull out a PR with a test. I'll be happy to merge it"], ["Recently I tried word2vec from dl4j, but I found some bugs! Firstly, [<-LINK->] in line 114 \"words.get(a).getPoints().add(words.size() - 2);\" here is not correct. The original code from google(c language) shows \"vocab[a].point[0] = vocab_size - 2;\" the meaning is very different. And I also do the experiment, it is really not correct. Also, I tried the spark version of word2vec, it seems problem is more serious, [<-LINK->] the biggest bug is line 151, \"indexSyn0VecMap.put(currentWordIndex, randomSyn0Vec);\" here should be \"w2.getIndex()\" instead of \"currentWordIndex\". Also, in line 114 \"INDArray randomSyn0Vec = getRandomSyn0Vec(vectorLength);\" here should not always create random vectors. it should be like this: [<-CODE->]. Also some other parts of spark version of word2vec are not very ideal. Anyway, I think spark in-built word2vec (scala) is also not good. Because it just use sum of local lookup_table to get global lookup_table directly. I think it should use mean or some other ways. So that's why I study these different version of word2vec.", "@whqwill File an issue.", "meh, Not sure what happened there, releasing can be weird sometimes."], ["@agibsonccc the class BaseOutputLayer, 425 line, bug: [<-CODE->]", "@cqiaoYc given our very thorough unit tests - I doubt that's a bug. got a specific example that it actually fails on?", "It should be obvious here : return labels.reshape(labels.size(0), labels.size(1)); //line 425", "that would be a no-op I think", "when i use \".inputPreProcessor(3, new RnnToFeedForwardPreProcessor())\", the label was Converted into 1D, my timeStep=1. but it can work in regression.", "@cqiaoYc post your full configuration in a !gist", "gist cant use in china, can report a bug? It looks like a BUG", "I'm not yet convinced - but sure, open an issue, and I'll take a look", "ok", "@AlexDBlack [<-ISSUE->]", "thanks"], ["@raver119 about the crash, I'm running the following 4 lines of (Kotlin) code in multiple threads:[<-CODE->]. I'm seeing the crash with openJdk8. I'm not 100% sure if it crashes with Oracle JDK as well. The crashing runs were a Spark job on EMR and they use openJDK. I can try Oracle locally and let you know in a few hours (takes quite a while which is why I wanted to parallelize).", "@oliverdain !issue please. ", "that stuff shouldn\u2019t pay attention to jvm being used. But \u00abspark\u00bb word makes it suspicious. how you\u2019re sending things to spark?", "The other big difference in that environment vs. my laptop is # of cores. I have only 2 (each hyperthreaded) but the Spark machines have 8. Wonder if that just makes it easier to trigger this.", "@oliverdain and if it crashes the jvm it is most probably a libnd4j bug or a problem between nd4j and libnd4j", "hmmmm.. It seems to have just passed the turing test :)", "Will file bug on nd4j then. Will try to reproduce on my laptop first. That'll take a few hours.", "File it even before reproducing, and add your results after that. It looks like your crash may be due to a race condition that only shows itself in a highly parallel scenario, so you can wait a long time before you see it on your laptop, if ever", "@treo OK. Will do."], ["0.015 Learning Rate:[<-LINK->]. 0.15 Learning Rate:[<-LINK->] I was looking into the RNN Regression example and I am a little confused by one of the results I got. The model reports on the MSE of the Test Dataset. When I run the model with a learning rate of 0.15, it has a MSE of 1.00571e+00 and the graph is nearly perfect. When I run the model with a learning rate of 0.015, it has a slight lower MSE of 9.55186e-01. I would expect the graph to be about the same or slightly better due to the lower MSE, but the graph with the model having a learning rate of 0.015 is worse. This just doesn't seem right to me, can anyone explain it?", "I just thought a lower MSE meant a better fit of the data, and I don't understand why that isn't true.", "@Bren077s so the model reports a loss which is also an MSE. The higher and lower numbers you are quoting, are they the loss?", "No they are not the loss of the model. They are the MSE when the model is evaluated on the test data. It is what is printed by the last line in this segment of code [<-CODE->]. I believe that these MSE numbers should directly correlate to performance on the test dataset.", "Ah, I bet the mae reported is opposite trend, Is it? Those stats are collected on the scaled output between zero and one.", "Both MAEs are extremely close(0.02 apart). But they are same trend. ", "Yes they are collected on the scaled output, ", "could that be causing the issue?", "No. That's not an issue.", "Okay, any idea why the MSE is lower with a worse result?", "It's just that it throws of the scale a bit. I have to look at the code. On my phone atm. I'll look and let you know.", "thank you", "This might be better as an !issue? We could frame this as something to verify with the GUI", "want me to report it? Also what do you mean by verify with the gui?", "The charts might be off for some reason? I mean it could be anything, frame it clearly as ambiguous. I'm just saying in general, it could be a scaling issue, the data. Neither of us is sure :D. Hence why she's investingating", "ok. Makes sense", "It's mostly \"one of my engineers is looking in to a possible bug and we aren't sure what, but this chart looks weird, we're not sure why\". \"Let's log it to acknowledge something is being done and also just remember that this happened\"", "It is a time series prediction example. The result at LR 0.0015 just happens to come closest to the actual result. @Bren077s @agibsonccc .", "Typically a lower MSE by definition is closer to the \"true\" though :D", "@RobAltena The question is \"If it has a lower MSE, why is it farther from the actual result?\"", "That seems like an obvious thing to have right", "@agibsonccc there is a permute in eval time series and ", "@Bren077s case might hit the corner case of tad with one dimension equals 1. There is only one feature in the time series.", "@Bren077s could be a known bug. ", "@Bren077s That Adam is working on", "subtle but possible", "Is the bug simply cause evalTimeSeries to incorrectly calculate the values? and is there a way for me to test and confirm if it is the bug?", "@Bren077s let's just file it for now and we'll double check this", "Sounds good with me. I will file it.", "thanks"], ["Um, not sure if this really is an issue or just a small bug, but i was running the GravesLSTMCharModellingExample, and while it was running it prints out the samples of text from shakespears work, but it seems to also include the Headers or Footers of the downloaded file. As some the examples have: [<-LINK->]. Its not the biggest issue or bug in the world, but i though i would mension it."], ["hello, ", "when i run the DBNfullexample by use IntelliJ IDEA, and i have the following problem [A fatal error has been detected by the Java Runtime Environment: [<-CODE->]. i try the following command in terminal [<-CODE->] then rerun the program but the problem not solved . what should i do to solve it?", "@FatimaAswadi Could you file an issue on  [<-LINK->] and give us literally EVERYTHING you can about the crash? OS, how you ran it, version of java Include that crash log in a github gist as well [<-LINK->] as well in another github gist", "the OS is Ubuntu 14.04 LTS and the java version is java version \"1.7.0_101\" OpenJDK Runtime Environment (IcedTea 2.6.6) (7u101-2.6.6-0ubuntu0.14.04.1) OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)", "@FatimaAswadi Github. Issue. I'm not here to dig in to your issue this millisecond. Please do exactly and only as I say. Nothing more or less. None of that matters right now. We will get to it when we get time. Like I said we also need that log and everything else in a github issue only not this chat", "ok , i will do", "thanks"]], "user": [["gforman44", "agibsonccc", "gforman44", "gforman44", "agibsonccc", "gforman44", "agibsonccc", "gforman44", "gforman44", "agibsonccc"], ["akhodakivskiy", "saudet", "akhodakivskiy", "saudet", "akhodakivskiy", "akhodakivskiy", "akhodakivskiy", "saudet", "akhodakivskiy", "akhodakivskiy", "agibsonccc", "akhodakivskiy", "raver119", "akhodakivskiy"], ["perceptoid", "agibsonccc", "perceptoid", "agibsonccc", "perceptoid", "perceptoid"], ["EdeMeijer", "agibsonccc", "EdeMeijer", "agibsonccc", "EdeMeijer", "EdeMeijer"], ["StillNoNumber", "raver119", "raver119", "raver119", "raver119", "StillNoNumber", "raver119", "StillNoNumber", "raver119", "StillNoNumber", "StillNoNumber", "StillNoNumber"], ["kuluofenghun"], ["app86", "app86", "agibsonccc", "raver119", "raver119", "raver119", "raver119", "app86", "raver119", "app86", "raver119", "app86"], ["enache2004", "daredemo", "enache2004", "enache2004", "raver119", "enache2004", "daredemo", "enache2004", "daredemo", "enache2004", "enache2004", "enache2004", "enache2004", "raver119", "raver119", "enache2004", "enache2004", "enache2004", "raver119", "raver119", "raver119", "raver119", "raver119", "enache2004", "enache2004", "raver119", "raver119", "enache2004"], ["namsor", "namsor", "namsor", "agibsonccc", "namsor", "agibsonccc"], ["cerisara", "cerisara", "agibsonccc"], ["whqwill", "agibsonccc", "agibsonccc"], ["cqiaoYc", "AlexDBlack", "cqiaoYc", "AlexDBlack", "cqiaoYc", "AlexDBlack", "cqiaoYc", "AlexDBlack", "cqiaoYc", "cqiaoYc", "AlexDBlack"], ["oliverdain", "raver119", "raver119", "oliverdain", "treo", "oliverdain", "oliverdain", "treo", "oliverdain"], ["Bren077s", "Bren077s", "eraly", "Bren077s", "eraly", "Bren077s", "Bren077s", "Bren077s", "eraly", "Bren077s", "eraly", "Bren077s", "agibsonccc", "Bren077s", "agibsonccc", "Bren077s", "agibsonccc", "RobAltena", "agibsonccc", "Bren077s", "agibsonccc", "eraly", "eraly", "eraly", "eraly", "agibsonccc", "Bren077s", "agibsonccc", "Bren077s", "agibsonccc"], ["gjossep"], ["FatimaAswadi", "FatimaAswadi", "agibsonccc", "FatimaAswadi", "agibsonccc", "FatimaAswadi", "agibsonccc"]], "dialog_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "sen_label": [["OQ", "RA", "AE", "NF", "AE", "AE", "AE", "AE", "GG", "AE"], ["OQ", "SA", "QE", "QA", "NF", "AE", "AE", "AE", "QE", "QE", "QA", "AE", "AE", "AE"], ["OQ", "SA", "QE", "QA", "AE", "NF"], ["OQ", "QA", "AE", "GG", "AE", "AE"], ["OQ", "QA", "AE", "AE", "JK", "QE", "QA", "AE", "AE", "AE", "AE", "JK"], ["OQ"], ["GG", "OQ", "RA", "AE", "AE", "JK", "QE", "QA", "RA", "PF", "RA", "PF"], ["OQ", "QE", "AE", "QA", "SA", "AE", "QE", "QE", "QA", "QA", "AE", "AE", "AE", "AE", "SA", "AE", "PF", "AE", "AE", "AE", "AE", "AE", "AE", "AE", "QE", "QA", "AE", "AE"], ["GG", "OQ", "GG", "RA", "PF", "GG"], ["OQ", "AE", "RA"], ["OQ", "RA", "UF"], ["OQ", "QE", "QA", "AE", "AE", "RA", "NF", "RA", "PF", "AE", "GG"], ["OQ", "RA", "QE", "QA", "AE", "AE", "AE", "SA", "UF"], ["OQ", "AE", "QE", "QA", "QE", "AE", "QA", "QE", "QA", "QE", "QA", "GG", "AE", "QE", "QA", "GG", "AE", "AE", "AE", "QE", "QA", "AE", "AE", "AE", "AE", "AE", "QE", "RA", "PF", "GG"], ["OQ"], ["GG", "OQ", "RA", "AE", "RA", "PF", "GG"]], "graph_edge": [[[0, 1], [0, 2], [1, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9]], [[0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [6, 11], [11, 12], [12, 13]], [[0, 1], [1, 2], [2, 3], [3, 4], [3, 5]], [[0, 1], [1, 2], [2, 3], [2, 4], [4, 5]], [[0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11]], [0], [[0, 1], [1, 2], [1, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11]], [[0, 1], [0, 2], [1, 3], [2, 4], [4, 5], [3, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11], [11, 12], [11, 13], [13, 14], [14, 15], [15, 16], [16, 17], [17, 18], [18, 19], [19, 20], [20, 21], [21, 22], [22, 23], [23, 24], [24, 25], [25, 26], [26, 27]], [[1, 2], [1, 3], [3, 4], [4, 5]], [[0, 1], [1, 2]], [[0, 1], [0, 2]], [[0, 1], [1, 2], [2, 3], [2, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10]], [[0, 1], [0, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8]], [[0, 1], [1, 2], [2, 3], [3, 4], [3, 5], [4, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11], [11, 12], [12, 13], [13, 14], [14, 15], [15, 16], [16, 17], [17, 18], [18, 19], [19, 20], [20, 21], [19, 22], [19, 23], [19, 24], [24, 25], [25, 26], [26, 27], [27, 28], [28, 29]], [0], [[1, 2], [1, 3], [3, 4], [4, 5], [5, 6]]]}